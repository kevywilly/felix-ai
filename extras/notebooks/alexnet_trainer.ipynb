{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-03-02T23:04:45.924280Z",
     "start_time": "2024-03-02T23:04:44.452508Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kevywilly/felix-ai/venv/lib/python3.9/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********************************\n",
      "*\tRobot = felixMac\n",
      "********************************\n",
      "\n",
      "****************************************\n",
      "*\tLoading App Settings\n",
      "****************************************\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "from src.training.datasets import CustomImageFolder\n",
    "import os\n",
    "from settings import settings\n",
    "import torchvision.models as models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kevywilly/felix-ai/venv/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/Users/kevywilly/felix-ai/venv/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=AlexNet_Weights.IMAGENET1K_V1`. You can also use `weights=AlexNet_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "data": {
      "text/plain": "AlexNet(\n  (features): Sequential(\n    (0): Conv2d(3, 64, kernel_size=(11, 11), stride=(4, 4), padding=(2, 2))\n    (1): ReLU(inplace=True)\n    (2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n    (3): Conv2d(64, 192, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n    (4): ReLU(inplace=True)\n    (5): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n    (6): Conv2d(192, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (7): ReLU(inplace=True)\n    (8): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (9): ReLU(inplace=True)\n    (10): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (11): ReLU(inplace=True)\n    (12): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n  )\n  (avgpool): AdaptiveAvgPool2d(output_size=(6, 6))\n  (classifier): Sequential(\n    (0): Dropout(p=0.5, inplace=False)\n    (1): Linear(in_features=9216, out_features=4096, bias=True)\n    (2): ReLU(inplace=True)\n    (3): Dropout(p=0.5, inplace=False)\n    (4): Linear(in_features=4096, out_features=4096, bias=True)\n    (5): ReLU(inplace=True)\n    (6): Linear(in_features=4096, out_features=3, bias=True)\n  )\n)"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = models.alexnet(pretrained=True)\n",
    "model.classifier[6] = torch.nn.Linear(model.classifier[6].in_features, 3)\n",
    "model.classifier[6].in_features\n",
    "model.train()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-02T23:04:46.199919Z",
     "start_time": "2024-03-02T23:04:45.928466Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "images_path=os.path.join(settings.TRAINING.data_root, \"training/ternary\")\n",
    "random_flip=True\n",
    "target_flips={1: 2, 2: 1}"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-02T23:04:46.203372Z",
     "start_time": "2024-03-02T23:04:46.201818Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "dataset = CustomImageFolder(\n",
    "            root=images_path,\n",
    "            target_flips=target_flips,\n",
    "            transform=transforms.Compose([\n",
    "                transforms.ColorJitter(0.1, 0.1, 0.1, 0.1),\n",
    "                transforms.Resize((224, 224)),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "            ],\n",
    "\n",
    "            )\n",
    "        )"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-02T23:04:46.209206Z",
     "start_time": "2024-03-02T23:04:46.204866Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "batch_size = 8\n",
    "test_pct = 20\n",
    "test_size = int(len(dataset)*test_pct/100.0)\n",
    "\n",
    "train_dataset, test_dataset = torch.utils.data.random_split(dataset, [len(dataset) - test_size, test_size])\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=8,\n",
    "    shuffle=True,\n",
    "    num_workers=0,\n",
    ")\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=128,\n",
    "    shuffle=True,\n",
    "    num_workers=0,\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-02T23:04:46.213908Z",
     "start_time": "2024-03-02T23:04:46.210683Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters())"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-02T23:04:46.217056Z",
     "start_time": "2024-03-02T23:04:46.215590Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 3, 224, 224])\n",
      "torch.Size([8, 3, 224, 224])\n",
      "tensor([[-0.5172,  0.5515,  1.1923],\n",
      "        [-0.3540, -0.7053,  0.1443],\n",
      "        [ 0.0201, -0.0664, -0.7616],\n",
      "        [ 0.1726,  0.3159, -0.8366],\n",
      "        [-0.8079,  1.0920, -0.2652],\n",
      "        [-0.4647,  1.4174,  0.0277],\n",
      "        [-0.7716, -0.5360, -0.1353],\n",
      "        [-0.1718,  0.6487, -0.7189]], grad_fn=<AddmmBackward0>)\n",
      "torch.Size([8, 3, 224, 224])\n",
      "torch.Size([8, 3, 224, 224])\n",
      "tensor([[ 1.0512,  0.0751,  1.2798],\n",
      "        [ 0.6715,  0.6542,  1.3594],\n",
      "        [-0.2765,  2.0656,  0.9120],\n",
      "        [ 0.5728,  0.3853, -0.0873],\n",
      "        [ 0.2543, -0.1882,  0.3223],\n",
      "        [ 0.4907,  0.8765, -0.4155],\n",
      "        [ 1.3118, -0.0622, -0.7604],\n",
      "        [ 0.6387, -0.5942,  1.6479]], grad_fn=<AddmmBackward0>)\n",
      "torch.Size([8, 3, 224, 224])\n",
      "torch.Size([8, 3, 224, 224])\n",
      "tensor([[-0.2760,  7.4486, -7.5615],\n",
      "        [ 0.4238,  6.1977, -5.7597],\n",
      "        [ 1.4301,  9.3256, -7.1679],\n",
      "        [ 1.0277,  8.3750, -7.9123],\n",
      "        [ 1.3044,  9.1237, -8.0052],\n",
      "        [ 0.2171,  7.7920, -7.5955],\n",
      "        [ 1.5085,  7.3514, -6.5246],\n",
      "        [ 0.3280,  7.5689, -7.3031]], grad_fn=<AddmmBackward0>)\n",
      "torch.Size([8, 3, 224, 224])\n",
      "torch.Size([8, 3, 224, 224])\n",
      "tensor([[ 2.0381, -3.4529,  1.8947],\n",
      "        [ 1.8535, -2.7351,  1.6985],\n",
      "        [ 2.7313, -3.1230,  1.2751],\n",
      "        [ 2.6022, -3.8626,  2.1315],\n",
      "        [ 2.6536, -3.4262,  1.2418],\n",
      "        [ 1.1955, -2.9628,  0.7497],\n",
      "        [ 2.7917, -2.7547,  2.1659],\n",
      "        [ 2.2942, -2.9691,  1.0937]], grad_fn=<AddmmBackward0>)\n",
      "torch.Size([8, 3, 224, 224])\n",
      "torch.Size([8, 3, 224, 224])\n",
      "tensor([[-0.7514,  0.0109,  1.7926],\n",
      "        [-1.3253,  0.2946,  1.6442],\n",
      "        [-1.6858,  0.5753,  3.3786],\n",
      "        [-1.7520,  0.6875,  2.9655],\n",
      "        [-1.0836, -0.0547,  2.7498],\n",
      "        [-2.1231,  0.9286,  3.0533],\n",
      "        [-1.1294,  0.7002,  2.4229],\n",
      "        [-1.4162,  0.6523,  2.2570]], grad_fn=<AddmmBackward0>)\n",
      "torch.Size([8, 3, 224, 224])\n",
      "torch.Size([8, 3, 224, 224])\n",
      "tensor([[ 1.2962,  1.5379, -1.5450],\n",
      "        [ 0.7545,  1.1491, -1.0929],\n",
      "        [ 1.4342,  2.1922, -2.8767],\n",
      "        [ 0.6536,  2.7618, -2.2876],\n",
      "        [ 1.5915,  2.2380, -3.0460],\n",
      "        [ 1.5712,  1.7675, -2.5452],\n",
      "        [ 1.0052,  3.3630, -2.8612],\n",
      "        [ 3.4138,  4.3087, -5.2957]], grad_fn=<AddmmBackward0>)\n",
      "torch.Size([8, 3, 224, 224])\n",
      "torch.Size([8, 3, 224, 224])\n",
      "tensor([[ 2.5055,  1.6519, -2.9484],\n",
      "        [ 2.3362,  1.2133, -2.0079],\n",
      "        [ 2.3525,  1.7545, -2.8719],\n",
      "        [ 2.1691,  1.9206, -2.7308],\n",
      "        [ 1.9794,  1.9302, -2.5398],\n",
      "        [ 2.4898,  2.5903, -3.0064],\n",
      "        [ 2.9736,  2.7224, -3.7825],\n",
      "        [ 3.4904,  2.8746, -3.9996]], grad_fn=<AddmmBackward0>)\n",
      "torch.Size([8, 3, 224, 224])\n",
      "torch.Size([8, 3, 224, 224])\n",
      "tensor([[ 0.9767,  1.4554, -0.9223],\n",
      "        [ 0.6697,  2.7416, -1.4220],\n",
      "        [ 1.7048,  2.4401, -1.5385],\n",
      "        [ 1.1514,  1.6229, -1.6709],\n",
      "        [ 1.1971,  2.3745, -1.2947],\n",
      "        [ 2.0575,  5.7907, -4.0986],\n",
      "        [ 1.3378,  2.3543, -1.4758],\n",
      "        [ 1.3892,  4.1732, -3.1146]], grad_fn=<AddmmBackward0>)\n",
      "torch.Size([8, 3, 224, 224])\n",
      "torch.Size([8, 3, 224, 224])\n",
      "tensor([[ 0.0801,  0.4254, -0.1822],\n",
      "        [ 0.1597,  0.6129, -0.4225],\n",
      "        [ 0.0442,  0.3139, -0.2005],\n",
      "        [ 0.1269,  0.4063, -0.2388],\n",
      "        [ 0.2659,  0.2373,  0.4855],\n",
      "        [ 0.0716,  0.2100, -0.0728],\n",
      "        [ 0.0972,  0.4488, -0.1492],\n",
      "        [ 0.1261,  0.2684, -0.1414]], grad_fn=<AddmmBackward0>)\n",
      "torch.Size([8, 3, 224, 224])\n",
      "torch.Size([8, 3, 224, 224])\n",
      "tensor([[ 0.1686, -0.2828,  0.3168],\n",
      "        [ 0.2066, -0.1320,  0.0647],\n",
      "        [ 0.1261, -0.1532,  0.1427],\n",
      "        [-0.3586,  0.0522,  0.1158],\n",
      "        [-0.1792, -0.0335,  0.1634],\n",
      "        [ 0.4152, -0.0206,  0.1238],\n",
      "        [ 0.2198, -0.0868,  0.2354],\n",
      "        [ 0.2679, -0.0165,  0.1112]], grad_fn=<AddmmBackward0>)\n",
      "torch.Size([8, 3, 224, 224])\n",
      "torch.Size([8, 3, 224, 224])\n",
      "tensor([[ 0.8940, -2.0183,  0.9326],\n",
      "        [ 0.7293, -1.5833,  0.6197],\n",
      "        [ 1.2650, -1.6732,  0.8697],\n",
      "        [ 1.7061, -2.8194,  1.1019],\n",
      "        [ 1.6111, -2.2392,  0.6589],\n",
      "        [ 0.6058, -1.5886,  0.7189],\n",
      "        [ 0.6887, -1.8548,  1.0527],\n",
      "        [ 1.1945, -2.0467,  1.1335]], grad_fn=<AddmmBackward0>)\n",
      "torch.Size([8, 3, 224, 224])\n",
      "torch.Size([8, 3, 224, 224])\n",
      "tensor([[ 3.9764, -2.8944, -0.3883],\n",
      "        [ 2.6933, -2.1404, -0.0924],\n",
      "        [ 2.3213, -1.7989, -0.0563],\n",
      "        [ 1.4571, -1.4196,  0.1920],\n",
      "        [ 4.0643, -3.7236, -0.0931],\n",
      "        [ 5.5847, -5.1217, -0.2320],\n",
      "        [ 1.7052, -1.5104,  0.1853],\n",
      "        [ 3.3074, -2.4966,  0.0264]], grad_fn=<AddmmBackward0>)\n",
      "torch.Size([8, 3, 224, 224])\n",
      "torch.Size([8, 3, 224, 224])\n",
      "tensor([[ 0.2443, -0.1870,  0.0582],\n",
      "        [ 0.2524, -0.1263,  0.0484],\n",
      "        [ 0.3971, -0.2468,  0.0793],\n",
      "        [ 0.4918, -0.2192, -0.0230],\n",
      "        [ 0.1758, -0.0079,  0.0804],\n",
      "        [ 0.2859, -0.0597,  0.0573],\n",
      "        [ 0.4335, -0.2526, -0.0718],\n",
      "        [ 0.1953, -0.1735,  0.0655]], grad_fn=<AddmmBackward0>)\n",
      "torch.Size([8, 3, 224, 224])\n",
      "torch.Size([8, 3, 224, 224])\n",
      "tensor([[ 0.3396,  0.0652, -0.0604],\n",
      "        [ 0.4865, -0.3729,  0.2264],\n",
      "        [ 0.2386, -0.0335,  0.0618],\n",
      "        [ 0.3485, -0.2992,  0.1910],\n",
      "        [ 0.4295, -0.3800,  0.1445],\n",
      "        [ 0.4104, -0.1324,  0.0841],\n",
      "        [ 0.1156,  0.0331,  0.0343],\n",
      "        [ 0.4666, -0.2155,  0.1515]], grad_fn=<AddmmBackward0>)\n",
      "torch.Size([8, 3, 224, 224])\n",
      "torch.Size([8, 3, 224, 224])\n",
      "tensor([[ 1.4552, -0.7289, -0.3446],\n",
      "        [ 1.1098, -0.7138, -0.3095],\n",
      "        [ 1.8129, -1.0806, -0.5041],\n",
      "        [ 1.1397, -0.4548, -0.4249],\n",
      "        [ 2.1951, -1.1184, -0.6015],\n",
      "        [ 1.7149, -0.7010, -0.6098],\n",
      "        [ 2.1681, -1.5325, -0.1043],\n",
      "        [ 1.5829, -0.9864, -0.1566]], grad_fn=<AddmmBackward0>)\n",
      "torch.Size([8, 3, 224, 224])\n",
      "torch.Size([8, 3, 224, 224])\n",
      "tensor([[ 0.3564, -0.0970, -0.0097],\n",
      "        [ 0.5278, -0.2420,  0.0664],\n",
      "        [ 0.4086, -0.2318,  0.0135],\n",
      "        [ 0.4265, -0.1834, -0.0073],\n",
      "        [ 0.2105, -0.0972,  0.0323],\n",
      "        [ 0.2811, -0.0301, -0.0120],\n",
      "        [ 0.2042, -0.1304, -0.0239],\n",
      "        [ 0.4315, -0.4121,  0.0919]], grad_fn=<AddmmBackward0>)\n",
      "torch.Size([8, 3, 224, 224])\n",
      "torch.Size([8, 3, 224, 224])\n",
      "tensor([[ 0.1522, -0.0685,  0.0027],\n",
      "        [ 0.3409, -0.1955,  0.0267],\n",
      "        [ 0.1446, -0.0943,  0.0229],\n",
      "        [ 0.3103, -0.1742, -0.0333],\n",
      "        [ 0.1967, -0.1219, -0.0093],\n",
      "        [ 0.2328, -0.1397,  0.0245],\n",
      "        [ 0.2169, -0.1506,  0.0129],\n",
      "        [ 0.3910, -0.1699, -0.0350]], grad_fn=<AddmmBackward0>)\n",
      "torch.Size([8, 3, 224, 224])\n",
      "torch.Size([8, 3, 224, 224])\n",
      "tensor([[ 0.1397, -0.1086,  0.0173],\n",
      "        [ 0.1141, -0.1017,  0.0603],\n",
      "        [ 0.1016, -0.0680,  0.0085],\n",
      "        [ 0.1533, -0.1227,  0.0475],\n",
      "        [ 0.0450, -0.0322,  0.0637],\n",
      "        [ 0.1184, -0.0874, -0.0288],\n",
      "        [ 0.1455, -0.1243, -0.0214],\n",
      "        [ 0.1537, -0.1163, -0.0054]], grad_fn=<AddmmBackward0>)\n",
      "torch.Size([8, 3, 224, 224])\n",
      "torch.Size([8, 3, 224, 224])\n",
      "tensor([[ 0.0627, -0.1853,  0.1109],\n",
      "        [ 0.0434, -0.1245,  0.1285],\n",
      "        [ 0.0557, -0.1656,  0.1444],\n",
      "        [ 0.0676, -0.1054,  0.0900],\n",
      "        [ 0.0744, -0.1756,  0.1118],\n",
      "        [ 0.0705, -0.0963,  0.0837],\n",
      "        [ 0.1179, -0.1519,  0.1021],\n",
      "        [ 0.0640, -0.1508,  0.1453]], grad_fn=<AddmmBackward0>)\n",
      "torch.Size([8, 3, 224, 224])\n",
      "torch.Size([8, 3, 224, 224])\n",
      "tensor([[-0.0060, -0.0960,  0.1464],\n",
      "        [-0.0018, -0.0851,  0.1305],\n",
      "        [ 0.0165, -0.0778,  0.1225],\n",
      "        [-0.0416, -0.1368,  0.1600],\n",
      "        [-0.0150, -0.1637,  0.1498],\n",
      "        [-0.0683, -0.1164,  0.1648],\n",
      "        [-0.0276, -0.1392,  0.1480],\n",
      "        [ 0.0004, -0.0931,  0.1437]], grad_fn=<AddmmBackward0>)\n",
      "torch.Size([8, 3, 224, 224])\n",
      "torch.Size([8, 3, 224, 224])\n",
      "tensor([[-0.1337, -0.1374,  0.2328],\n",
      "        [ 0.0779, -0.0846,  0.0851],\n",
      "        [-0.1213, -0.1305,  0.2299],\n",
      "        [-0.1580, -0.1473,  0.2659],\n",
      "        [-0.1350, -0.1550,  0.2637],\n",
      "        [-0.0466, -0.0835,  0.1746],\n",
      "        [-0.1433, -0.1361,  0.2482],\n",
      "        [-0.1198, -0.1242,  0.2300]], grad_fn=<AddmmBackward0>)\n",
      "torch.Size([8, 3, 224, 224])\n",
      "torch.Size([8, 3, 224, 224])\n",
      "tensor([[-0.2139, -0.0842,  0.2581],\n",
      "        [-0.1763, -0.0357,  0.2140],\n",
      "        [-0.1069, -0.0410,  0.2144],\n",
      "        [-0.2120, -0.0894,  0.2640],\n",
      "        [-0.1853, -0.0857,  0.2339],\n",
      "        [-0.1412,  0.0039,  0.1678],\n",
      "        [-0.1711, -0.0244,  0.2183],\n",
      "        [-0.2016, -0.0903,  0.2672]], grad_fn=<AddmmBackward0>)\n",
      "torch.Size([8, 3, 224, 224])\n",
      "torch.Size([8, 3, 224, 224])\n",
      "tensor([[-0.0280, -0.0077,  0.0904],\n",
      "        [-0.1173,  0.0227,  0.1387],\n",
      "        [-0.0817,  0.0183,  0.1072],\n",
      "        [-0.0591,  0.0004,  0.1175],\n",
      "        [-0.1006, -0.0466,  0.1488],\n",
      "        [-0.0784,  0.0487,  0.1002],\n",
      "        [-0.0684, -0.0321,  0.1400],\n",
      "        [-0.0928,  0.0083,  0.1030]], grad_fn=<AddmmBackward0>)\n",
      "torch.Size([8, 3, 224, 224])\n",
      "torch.Size([8, 3, 224, 224])\n",
      "tensor([[ 0.1275, -0.0436,  0.0696],\n",
      "        [-0.0254,  0.0545,  0.0207],\n",
      "        [ 0.0744, -0.0011,  0.0861],\n",
      "        [ 0.0666,  0.0170,  0.0549],\n",
      "        [ 0.0110, -0.0068,  0.1040],\n",
      "        [ 0.0338, -0.1420,  0.3190],\n",
      "        [ 0.0966, -0.1020,  0.1587],\n",
      "        [ 0.0868, -0.0873,  0.0849]], grad_fn=<AddmmBackward0>)\n",
      "torch.Size([3, 3, 224, 224])\n",
      "torch.Size([3, 3, 224, 224])\n",
      "tensor([[-0.0259,  0.1183, -0.0227],\n",
      "        [-0.2166,  0.6224, -0.2394],\n",
      "        [-0.1758,  0.5717, -0.2306]], grad_fn=<AddmmBackward0>)\n",
      "Epoch 1/10, Train Loss: 1.7222, Test Loss: 1.1046\n",
      "torch.Size([8, 3, 224, 224])\n",
      "torch.Size([8, 3, 224, 224])\n",
      "tensor([[ 0.0810,  0.0745, -0.0652],\n",
      "        [-0.0590,  0.0441,  0.0732],\n",
      "        [ 0.0872, -0.0658,  0.0581],\n",
      "        [-0.0420,  0.0814,  0.0395],\n",
      "        [-0.0013, -0.0183,  0.0788],\n",
      "        [-0.0433, -0.0311,  0.0839],\n",
      "        [-0.0681,  0.0220,  0.0849],\n",
      "        [-0.0060, -0.0243,  0.1226]], grad_fn=<AddmmBackward0>)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[7], line 9\u001B[0m\n\u001B[1;32m      7\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m it \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(n_epochs):\n\u001B[1;32m      8\u001B[0m   train_loss \u001B[38;5;241m=\u001B[39m []\n\u001B[0;32m----> 9\u001B[0m   \u001B[38;5;28;01mfor\u001B[39;00m inputs, targets \u001B[38;5;129;01min\u001B[39;00m train_loader:\n\u001B[1;32m     10\u001B[0m     \u001B[38;5;66;03m# move data to GPU\u001B[39;00m\n\u001B[1;32m     13\u001B[0m     \u001B[38;5;28mprint\u001B[39m(inputs\u001B[38;5;241m.\u001B[39mshape)\n\u001B[1;32m     14\u001B[0m     \u001B[38;5;66;03m# reshape the input\u001B[39;00m\n\u001B[1;32m     15\u001B[0m     \u001B[38;5;66;03m#inputs = inputs.view(-1, 224,224)\u001B[39;00m\n",
      "File \u001B[0;32m~/felix-ai/venv/lib/python3.9/site-packages/torch/utils/data/dataloader.py:631\u001B[0m, in \u001B[0;36m_BaseDataLoaderIter.__next__\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    628\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_sampler_iter \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    629\u001B[0m     \u001B[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001B[39;00m\n\u001B[1;32m    630\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_reset()  \u001B[38;5;66;03m# type: ignore[call-arg]\u001B[39;00m\n\u001B[0;32m--> 631\u001B[0m data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_next_data\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    632\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_num_yielded \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[1;32m    633\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_dataset_kind \u001B[38;5;241m==\u001B[39m _DatasetKind\u001B[38;5;241m.\u001B[39mIterable \u001B[38;5;129;01mand\u001B[39;00m \\\n\u001B[1;32m    634\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_IterableDataset_len_called \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m \\\n\u001B[1;32m    635\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_num_yielded \u001B[38;5;241m>\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_IterableDataset_len_called:\n",
      "File \u001B[0;32m~/felix-ai/venv/lib/python3.9/site-packages/torch/utils/data/dataloader.py:675\u001B[0m, in \u001B[0;36m_SingleProcessDataLoaderIter._next_data\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    673\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_next_data\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[1;32m    674\u001B[0m     index \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_next_index()  \u001B[38;5;66;03m# may raise StopIteration\u001B[39;00m\n\u001B[0;32m--> 675\u001B[0m     data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_dataset_fetcher\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfetch\u001B[49m\u001B[43m(\u001B[49m\u001B[43mindex\u001B[49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# may raise StopIteration\u001B[39;00m\n\u001B[1;32m    676\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_pin_memory:\n\u001B[1;32m    677\u001B[0m         data \u001B[38;5;241m=\u001B[39m _utils\u001B[38;5;241m.\u001B[39mpin_memory\u001B[38;5;241m.\u001B[39mpin_memory(data, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_pin_memory_device)\n",
      "File \u001B[0;32m~/felix-ai/venv/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py:49\u001B[0m, in \u001B[0;36m_MapDatasetFetcher.fetch\u001B[0;34m(self, possibly_batched_index)\u001B[0m\n\u001B[1;32m     47\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mauto_collation:\n\u001B[1;32m     48\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m__getitems__\u001B[39m\u001B[38;5;124m\"\u001B[39m) \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset\u001B[38;5;241m.\u001B[39m__getitems__:\n\u001B[0;32m---> 49\u001B[0m         data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdataset\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m__getitems__\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpossibly_batched_index\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     50\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m     51\u001B[0m         data \u001B[38;5;241m=\u001B[39m [\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset[idx] \u001B[38;5;28;01mfor\u001B[39;00m idx \u001B[38;5;129;01min\u001B[39;00m possibly_batched_index]\n",
      "File \u001B[0;32m~/felix-ai/venv/lib/python3.9/site-packages/torch/utils/data/dataset.py:399\u001B[0m, in \u001B[0;36mSubset.__getitems__\u001B[0;34m(self, indices)\u001B[0m\n\u001B[1;32m    397\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset\u001B[38;5;241m.\u001B[39m__getitems__([\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mindices[idx] \u001B[38;5;28;01mfor\u001B[39;00m idx \u001B[38;5;129;01min\u001B[39;00m indices])  \u001B[38;5;66;03m# type: ignore[attr-defined]\u001B[39;00m\n\u001B[1;32m    398\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m--> 399\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m [\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset[\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mindices[idx]] \u001B[38;5;28;01mfor\u001B[39;00m idx \u001B[38;5;129;01min\u001B[39;00m indices]\n",
      "File \u001B[0;32m~/felix-ai/venv/lib/python3.9/site-packages/torch/utils/data/dataset.py:399\u001B[0m, in \u001B[0;36m<listcomp>\u001B[0;34m(.0)\u001B[0m\n\u001B[1;32m    397\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset\u001B[38;5;241m.\u001B[39m__getitems__([\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mindices[idx] \u001B[38;5;28;01mfor\u001B[39;00m idx \u001B[38;5;129;01min\u001B[39;00m indices])  \u001B[38;5;66;03m# type: ignore[attr-defined]\u001B[39;00m\n\u001B[1;32m    398\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m--> 399\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m [\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdataset\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mindices\u001B[49m\u001B[43m[\u001B[49m\u001B[43midx\u001B[49m\u001B[43m]\u001B[49m\u001B[43m]\u001B[49m \u001B[38;5;28;01mfor\u001B[39;00m idx \u001B[38;5;129;01min\u001B[39;00m indices]\n",
      "File \u001B[0;32m~/felix-ai/src/training/datasets.py:44\u001B[0m, in \u001B[0;36mCustomImageFolder.__getitem__\u001B[0;34m(self, index)\u001B[0m\n\u001B[1;32m     41\u001B[0m             target \u001B[38;5;241m=\u001B[39m tf\n\u001B[1;32m     43\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtransform \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m---> 44\u001B[0m     sample \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtransform\u001B[49m\u001B[43m(\u001B[49m\u001B[43msample\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     45\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtarget_transform \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m     46\u001B[0m     target \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtarget_transform(target)\n",
      "File \u001B[0;32m~/felix-ai/venv/lib/python3.9/site-packages/torchvision/transforms/transforms.py:95\u001B[0m, in \u001B[0;36mCompose.__call__\u001B[0;34m(self, img)\u001B[0m\n\u001B[1;32m     93\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__call__\u001B[39m(\u001B[38;5;28mself\u001B[39m, img):\n\u001B[1;32m     94\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m t \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtransforms:\n\u001B[0;32m---> 95\u001B[0m         img \u001B[38;5;241m=\u001B[39m \u001B[43mt\u001B[49m\u001B[43m(\u001B[49m\u001B[43mimg\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     96\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m img\n",
      "File \u001B[0;32m~/felix-ai/venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1511\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1509\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1510\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1511\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/felix-ai/venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1520\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1515\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1516\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1517\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1518\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1519\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1520\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1522\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m   1523\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[0;32m~/felix-ai/venv/lib/python3.9/site-packages/torchvision/transforms/transforms.py:1280\u001B[0m, in \u001B[0;36mColorJitter.forward\u001B[0;34m(self, img)\u001B[0m\n\u001B[1;32m   1278\u001B[0m         img \u001B[38;5;241m=\u001B[39m F\u001B[38;5;241m.\u001B[39madjust_saturation(img, saturation_factor)\n\u001B[1;32m   1279\u001B[0m     \u001B[38;5;28;01melif\u001B[39;00m fn_id \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m3\u001B[39m \u001B[38;5;129;01mand\u001B[39;00m hue_factor \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m-> 1280\u001B[0m         img \u001B[38;5;241m=\u001B[39m \u001B[43mF\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43madjust_hue\u001B[49m\u001B[43m(\u001B[49m\u001B[43mimg\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mhue_factor\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1282\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m img\n",
      "File \u001B[0;32m~/felix-ai/venv/lib/python3.9/site-packages/torchvision/transforms/functional.py:953\u001B[0m, in \u001B[0;36madjust_hue\u001B[0;34m(img, hue_factor)\u001B[0m\n\u001B[1;32m    951\u001B[0m     _log_api_usage_once(adjust_hue)\n\u001B[1;32m    952\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(img, torch\u001B[38;5;241m.\u001B[39mTensor):\n\u001B[0;32m--> 953\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mF_pil\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43madjust_hue\u001B[49m\u001B[43m(\u001B[49m\u001B[43mimg\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mhue_factor\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    955\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m F_t\u001B[38;5;241m.\u001B[39madjust_hue(img, hue_factor)\n",
      "File \u001B[0;32m~/felix-ai/venv/lib/python3.9/site-packages/torchvision/transforms/_functional_pil.py:109\u001B[0m, in \u001B[0;36madjust_hue\u001B[0;34m(img, hue_factor)\u001B[0m\n\u001B[1;32m    106\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m input_mode \u001B[38;5;129;01min\u001B[39;00m {\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mL\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m1\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mI\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mF\u001B[39m\u001B[38;5;124m\"\u001B[39m}:\n\u001B[1;32m    107\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m img\n\u001B[0;32m--> 109\u001B[0m h, s, v \u001B[38;5;241m=\u001B[39m \u001B[43mimg\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mconvert\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mHSV\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[38;5;241m.\u001B[39msplit()\n\u001B[1;32m    111\u001B[0m np_h \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39marray(h, dtype\u001B[38;5;241m=\u001B[39mnp\u001B[38;5;241m.\u001B[39muint8)\n\u001B[1;32m    112\u001B[0m \u001B[38;5;66;03m# uint8 addition take cares of rotation across boundaries\u001B[39;00m\n",
      "File \u001B[0;32m~/felix-ai/venv/lib/python3.9/site-packages/PIL/Image.py:1068\u001B[0m, in \u001B[0;36mImage.convert\u001B[0;34m(self, mode, matrix, dither, palette, colors)\u001B[0m\n\u001B[1;32m   1065\u001B[0m     dither \u001B[38;5;241m=\u001B[39m Dither\u001B[38;5;241m.\u001B[39mFLOYDSTEINBERG\n\u001B[1;32m   1067\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m-> 1068\u001B[0m     im \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mim\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mconvert\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmode\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdither\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1069\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m:\n\u001B[1;32m   1070\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m   1071\u001B[0m         \u001B[38;5;66;03m# normalize source image and try again\u001B[39;00m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "n_epochs = 10\n",
    "\n",
    "# Stuff to store\n",
    "train_losses = np.zeros(n_epochs)\n",
    "test_losses = np.zeros(n_epochs)\n",
    "\n",
    "for it in range(n_epochs):\n",
    "  train_loss = []\n",
    "  for inputs, targets in train_loader:\n",
    "    # move data to GPU\n",
    "\n",
    "\n",
    "    print(inputs.shape)\n",
    "    # reshape the input\n",
    "    #inputs = inputs.view(-1, 224,224)\n",
    "    print(inputs.shape)\n",
    "\n",
    "    # zero the parameter gradients\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Forward pass\n",
    "    outputs = model.forward(inputs)\n",
    "    print(outputs)\n",
    "    loss = criterion(outputs, targets)\n",
    "\n",
    "    # Backward and optimize\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    train_loss.append(loss.item())\n",
    "\n",
    "  # Get train loss and test loss\n",
    "  train_loss = np.mean(train_loss) # a little misleading\n",
    "\n",
    "  test_loss = []\n",
    "  for inputs, targets in test_loader:\n",
    "    #inputs = inputs.view(-1, 224,224)\n",
    "    outputs = model.forward(inputs)\n",
    "    loss = criterion(outputs, targets)\n",
    "    test_loss.append(loss.item())\n",
    "  test_loss = np.mean(test_loss)\n",
    "\n",
    "  # Save losses\n",
    "  train_losses[it] = train_loss\n",
    "  test_losses[it] = test_loss\n",
    "\n",
    "  print(f'Epoch {it+1}/{n_epochs}, Train Loss: {train_loss:.4f}, Test Loss: {test_loss:.4f}')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-02T23:05:18.378112Z",
     "start_time": "2024-03-02T23:04:46.220471Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
